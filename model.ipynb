{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e5da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Original column names: ['Item Fat Content', 'Item Identifier', 'Item Type', 'Outlet Establishment Year', 'Outlet Identifier', 'Outlet Location Type', 'Outlet Size', 'Outlet Type', 'Item Visibility', 'Item Weight', 'Sales', 'Rating']\n",
      "\n",
      "--- Cleaning Data ---\n",
      "Original 'Item Fat Content' values: ['Regular' 'Low Fat' 'low fat' 'LF' 'reg']\n",
      "Cleaned 'Item Fat Content' values: ['Regular' 'Low Fat']\n",
      "'Outlet Age' feature created.\n",
      "Dropped columns: ['Item Identifier', 'Outlet Identifier', 'Outlet Establishment Year']\n",
      "\n",
      "Predicting 'Sales' using features: ['Item Fat Content', 'Item Type', 'Outlet Location Type', 'Outlet Size', 'Outlet Type', 'Item Visibility', 'Item Weight', 'Rating', 'Outlet Age']\n",
      "\n",
      "--- Setting up Preprocessing ---\n",
      "'Outlet Size' identified as categorical.\n",
      "Categorical Features: ['Item Fat Content', 'Item Type', 'Outlet Location Type', 'Outlet Size', 'Outlet Type']\n",
      "Numerical Features: ['Item Visibility', 'Item Weight', 'Rating', 'Outlet Age']\n",
      "\n",
      "Data split into Train (6818 samples) and Test (1705 samples)\n",
      "\n",
      "--- Training Linear Regression ---\n",
      "Linear Regression training complete.\n",
      "\n",
      "--- Training Random Forest Regressor ---\n",
      "Random Forest training complete.\n",
      "\n",
      "--- Model Evaluation ---\n",
      "\n",
      "Linear Regression:\n",
      "  R-squared (R2): 0.0155\n",
      "  Mean Absolute Error (MAE): 52.33\n",
      "  Root Mean Squared Error (RMSE): 62.45\n",
      "\n",
      "Random Forest:\n",
      "  R-squared (R2): 0.3216\n",
      "  Mean Absolute Error (MAE): 42.13\n",
      "  Root Mean Squared Error (RMSE): 51.84\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "                       MAE        MSE     RMSE      R2\n",
      "Linear Regression  52.3276  3899.9362  62.4495  0.0155\n",
      "Random Forest      42.1298  2687.4871  51.8410  0.3216\n",
      "\n",
      "Model with the best R-squared score on the test set: Random Forest\n",
      "Linear Regression model pipeline saved successfully to 'linear_regression_sales_model.joblib'\n",
      "Random Forest model pipeline saved successfully to 'random_forest_sales_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "file_path = \"blinkit_grocery_data_orignal.xlsx\"  # <--- Make sure this path is correct\n",
    "try:\n",
    "    # When reading, pandas handles spaces in column names automatically\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    # Strip any leading/trailing whitespace from column names just in case\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(\"Original column names:\", df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Basic Cleaning & Feature Engineering ---\n",
    "print(\"\\n--- Cleaning Data ---\")\n",
    "\n",
    "# Standardize 'Item Fat Content'\n",
    "print(\"Original 'Item Fat Content' values:\", df[\"Item Fat Content\"].unique())\n",
    "df[\"Item Fat Content\"] = df[\"Item Fat Content\"].replace(\n",
    "    {\"LF\": \"Low Fat\", \"low fat\": \"Low Fat\", \"reg\": \"Regular\"}\n",
    ")\n",
    "print(\"Cleaned 'Item Fat Content' values:\", df[\"Item Fat Content\"].unique())\n",
    "\n",
    "# Feature Engineering: Extract Outlet Age\n",
    "current_year = pd.Timestamp.now().year\n",
    "if \"Outlet Establishment Year\" in df.columns:\n",
    "    df[\"Outlet Age\"] = current_year - df[\"Outlet Establishment Year\"]\n",
    "    print(\"'Outlet Age' feature created.\")\n",
    "else:\n",
    "    print(\"Warning: 'Outlet Establishment Year' not found, cannot create 'Outlet Age'.\")\n",
    "\n",
    "\n",
    "# --- 3. Feature Selection ---\n",
    "# Drop identifiers and the original year column\n",
    "# 'Item Identifier' and 'Outlet Identifier' usually don't help predict sales directly\n",
    "# and have too many unique values (high cardinality) for simple encoding.\n",
    "features_to_drop = [\"Item Identifier\", \"Outlet Identifier\"]\n",
    "# Only drop Outlet Establishment Year if Outlet Age was created\n",
    "if \"Outlet Age\" in df.columns:\n",
    "    features_to_drop.append(\"Outlet Establishment Year\")\n",
    "\n",
    "# Check which columns actually exist before dropping\n",
    "existing_cols_to_drop = [col for col in features_to_drop if col in df.columns]\n",
    "df.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "print(f\"Dropped columns: {existing_cols_to_drop}\")\n",
    "\n",
    "# Define target and features\n",
    "TARGET = \"Sales\"\n",
    "FEATURES = [col for col in df.columns if col != TARGET]\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "print(f\"\\nPredicting '{TARGET}' using features: {FEATURES}\")\n",
    "\n",
    "# --- 4. Preprocessing ---\n",
    "print(\"\\n--- Setting up Preprocessing ---\")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Check if 'Outlet Size' is categorical and handle its order if needed\n",
    "if \"Outlet Size\" in categorical_features:\n",
    "    print(\"'Outlet Size' identified as categorical.\")\n",
    "    # Note: If treating as ordinal, you might use OrdinalEncoder separately\n",
    "    # For simplicity here, we'll use OneHotEncoder which works for nominal/ordinal.\n",
    "\n",
    "print(f\"Categorical Features: {categorical_features}\")\n",
    "print(f\"Numerical Features: {numerical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "\n",
    "# Numerical Pipeline:\n",
    "# 1. Impute missing values (e.g., 'Item Weight', 'Rating') with the median\n",
    "# 2. Scale features to have zero mean and unit variance\n",
    "numerical_pipeline = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# Categorical Pipeline:\n",
    "# 1. Impute missing values (e.g., 'Outlet Size') with a constant value like 'Missing' or the mode\n",
    "# 2. One-Hot Encode categorical variables\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(strategy=\"most_frequent\"),\n",
    "        ),  # Or strategy='constant', fill_value='Missing'\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "        ),  # handle_unknown ignores categories only seen in test data\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, numerical_features),\n",
    "        (\"cat\", categorical_pipeline, categorical_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",  # Keep any columns not specified (shouldn't be any here)\n",
    ")\n",
    "\n",
    "# --- 5. Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\n",
    "    f\"\\nData split into Train ({X_train.shape[0]} samples) and Test ({X_test.shape[0]} samples)\"\n",
    ")\n",
    "\n",
    "# --- 6. Model Training ---\n",
    "# We'll try two models: Linear Regression and RandomForestRegressor\n",
    "\n",
    "# --- Model 1: Linear Regression ---\n",
    "print(\"\\n--- Training Linear Regression ---\")\n",
    "lr_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", LinearRegression())]\n",
    ")\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Linear Regression training complete.\")\n",
    "\n",
    "# --- Model 2: Random Forest Regressor ---\n",
    "print(\"\\n--- Training Random Forest Regressor ---\")\n",
    "# For Random Forest, scaling is less critical, but the pipeline structure is convenient\n",
    "rf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # n_estimators=100 is a common default, n_jobs=-1 uses all available CPU cores\n",
    "        (\n",
    "            \"regressor\",\n",
    "            RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "            ),\n",
    "        ),  # Added some hyperparameters\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "print(\"Random Forest training complete.\")\n",
    "\n",
    "\n",
    "# --- 7. Evaluation ---\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "\n",
    "models = {\"Linear Regression\": lr_pipeline, \"Random Forest\": rf_pipeline}\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  R-squared (R2): {r2:.4f}\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "results_df = pd.DataFrame(results).T  # Transpose for better viewing\n",
    "print(results_df.round(4))\n",
    "\n",
    "best_r2_model = results_df[\"R2\"].idxmax()\n",
    "print(f\"\\nModel with the best R-squared score on the test set: {best_r2_model}\")\n",
    "\n",
    "lr_model_filename = \"linear_regression_sales_model.joblib\"\n",
    "rf_model_filename = \"random_forest_sales_model.joblib\"\n",
    "\n",
    "# Save the Linear Regression pipeline (preprocessor + model)\n",
    "try:\n",
    "    joblib.dump(lr_pipeline, lr_model_filename)\n",
    "    print(\n",
    "        f\"Linear Regression model pipeline saved successfully to '{lr_model_filename}'\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Linear Regression model: {e}\")\n",
    "\n",
    "# Save the Random Forest pipeline (preprocessor + model)\n",
    "try:\n",
    "    joblib.dump(rf_pipeline, rf_model_filename)\n",
    "    print(f\"Random Forest model pipeline saved successfully to '{rf_model_filename}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Random Forest model: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
